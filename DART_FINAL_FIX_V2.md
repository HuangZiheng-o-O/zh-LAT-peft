# DART éªŒè¯é›†ä¸ºç©ºé—®é¢˜ - æœ€ç»ˆä¿®å¤

**æ—¶é—´**: 2025-11-08  
**é—®é¢˜**: è®­ç»ƒæ­£å¸¸ï¼Œè¯„ä¼°æ—¶å´©æºƒ `'NoneType' object is not subscriptable`  
**æ ¹æœ¬åŸå› **: éªŒè¯é›†ç¼“å­˜ä¸ºç©ºï¼ˆæ‰€æœ‰æ ·æœ¬å¤„ç†å¤±è´¥ï¼‰

---

## é—®é¢˜è¯Šæ–­

### é”™è¯¯ä¿¡æ¯

```
File "trainer/mamba_trainer.py", line 150, in generation_step
    input_ids, label_ids = inputs["input_ids"], inputs["label_ids"]
TypeError: 'NoneType' object is not subscriptable
```

### æ ¹æœ¬åŸå› 

1. **éªŒè¯é›†ç¼“å­˜ä¸ºç©º**ï¼š`self.data = []`
2. **DataLoader è¿”å› None**ï¼šç©ºæ•°æ®é›†å¯¼è‡´ `inputs = None`
3. **æ‰€æœ‰æ ·æœ¬å¤„ç†å¤±è´¥**ï¼š`get_input_label()` å¯¹æ‰€æœ‰æ ·æœ¬æŠ›å‡ºå¼‚å¸¸æˆ–è¿”å›æ— æ•ˆæ•°æ®

### æ·±å±‚åŸå› 

**é—®é¢˜ 1**: `to_str_list()` å‡½æ•°ï¼ˆ`dart_data.py` ç¬¬ 263-275 è¡Œï¼‰æ— æ³•å¤„ç†åµŒå¥—åˆ—è¡¨/numpy æ•°ç»„

```python
# æ—§ç‰ˆï¼ˆé”™è¯¯ï¼‰
def to_str_list(x):
    if isinstance(x, list):
        out = []
        for e in x:
            if isinstance(e, (str, int, float)) or e is None:  # â† åµŒå¥—åˆ—è¡¨è¢«è·³è¿‡
                s = "" if e is None else str(e)
                if s.strip() != "":
                    out.append(s)
        return out
    # ...
```

å½“ `build_lists()` è¿”å› `sources = [['WikiTableQuestions_mturk']]`ï¼ˆåµŒå¥—åˆ—è¡¨ï¼‰æ—¶ï¼Œ`to_str_list()` æ— æ³•æå–å†…å±‚å­—ç¬¦ä¸²ï¼Œå¯¼è‡´ `text` åˆ—å˜æˆç©ºåˆ—è¡¨ã€‚

**é—®é¢˜ 2**: `get_input_label()` ä¸­çš„å¼‚å¸¸å¤„ç†ï¼ˆç¬¬ 392-393 è¡Œï¼‰

```python
if len(text) == 0:
    raise ValueError(f"Sample {idx} has no valid text references after filtering")
```

å½“ `text` ä¸ºç©ºæ—¶æŠ›å‡ºå¼‚å¸¸ï¼Œå¯¼è‡´ `preproc()` å¤±è´¥ï¼Œæ ·æœ¬è¢«æ ‡è®°ä¸º `None`ã€‚å¦‚æœæ‰€æœ‰æ ·æœ¬éƒ½å¤±è´¥ï¼Œç¼“å­˜å˜æˆç©ºåˆ—è¡¨ã€‚

---

## ä¿®å¤æ–¹æ¡ˆ

### ä¿®å¤ 1: å¢å¼º `to_str_list()` å¤„ç†åµŒå¥—ç»“æ„

**æ–‡ä»¶**: `mamba-peft/dataset/dart_data.py`  
**ä½ç½®**: ç¬¬ 263-289 è¡Œ

```python
# Ensure list[str] for both columns (hardened)
def to_str_list(x):
    # Handle numpy arrays first
    if isinstance(x, np.ndarray):
        x = x.tolist()
    
    if isinstance(x, list):
        out = []
        for e in x:
            # Recursively handle nested structures
            if isinstance(e, (list, np.ndarray)):
                # Flatten one level
                for sub_e in (e.tolist() if isinstance(e, np.ndarray) else e):
                    if isinstance(sub_e, (str, int, float)) or sub_e is None:
                        s = "" if sub_e is None else str(sub_e)
                        if s.strip() != "":
                            out.append(s)
            elif isinstance(e, (str, int, float)) or e is None:
                s = "" if e is None else str(e)
                if s.strip() != "":
                    out.append(s)
        return out
    if isinstance(x, (str, int, float)) or x is None:
        s = "" if x is None else str(x)
        return [s] if s.strip() != "" else []
    return []
out["source"] = out.get("source", pd.Series([[]] * len(out))).apply(to_str_list)
out["text"]   = out.get("text",   pd.Series([[]] * len(out))).apply(to_str_list)
```

**æ”¹è¿›ç‚¹**ï¼š
- é€’å½’å¤„ç†åµŒå¥—åˆ—è¡¨å’Œ numpy æ•°ç»„
- å±•å¹³ä¸€å±‚åµŒå¥—
- ç¡®ä¿æœ€ç»ˆè¿”å› `list[str]`

### ä¿®å¤ 2: æ”¹è¿› `get_input_label()` é”™è¯¯å¤„ç†

**æ–‡ä»¶**: `mamba-peft/dataset/dart_data.py`  
**ä½ç½®**: ç¬¬ 387-429 è¡Œ

```python
else:
    # need to handle multiple references (generation mode)
    # Ensure source and text are lists (not numpy arrays)
    if isinstance(source, np.ndarray):
        source = source.tolist()
    if isinstance(text, np.ndarray):
        text = text.tolist()
    
    # Ensure they are lists
    if not isinstance(source, list):
        source = [source] if source else []
    if not isinstance(text, list):
        text = [text] if text else []
    
    # Flatten nested lists (defensive)
    def flatten_once(lst):
        result = []
        for item in lst:
            if isinstance(item, (list, np.ndarray)):
                result.extend(item.tolist() if isinstance(item, np.ndarray) else item)
            else:
                result.append(item)
        return result
    
    text = flatten_once(text)
    source = flatten_once(source)
    
    # Filter out any non-string elements
    text = [str(t).strip() for t in text if t is not None and str(t).strip()]
    
    if len(text) == 0:
        # Don't raise, return None so preproc filters it out
        print(f"[DART] Warning: Sample {idx} has no valid text after filtering, skipping")
        return None, None
    
    # Check for sep_token collision
    if any(self.sep_token in t for t in text):
        print(f"[DART] Warning: Sample {idx} contains sep_token '{self.sep_token}', replacing with space")
        text = [t.replace(self.sep_token, " ") for t in text]
    
    label = self.sep_token.join(text)

return input, label
```

**æ”¹è¿›ç‚¹**ï¼š
- æ·»åŠ  `flatten_once()` å‡½æ•°å¤„ç†åµŒå¥—åˆ—è¡¨
- ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œæ˜¯è¿”å› `(None, None)`
- å°† sep_token å†²çªä»æ–­è¨€æ”¹ä¸ºè­¦å‘Š+æ›¿æ¢

### ä¿®å¤ 3: åœ¨ `base.py` ä¸­å¤„ç† None è¿”å›å€¼

**æ–‡ä»¶**: `mamba-peft/dataset/base.py`  
**ä½ç½®**: ç¬¬ 111-124 è¡Œ

```python
def preproc(self, idx):
    input, label = self.get_input_label(idx)
    
    # Handle case where get_input_label returns (None, None) for invalid samples
    if input is None or label is None:
        return None
    
    input_prepoc, label_preproc = self.preproc_input_label(input, label)
    input_ids, label_ids = self.encode(input_prepoc), self.encode(label_preproc)

    if self.max_seqlen is not None and (input_ids.shape[0] + label_ids.shape[0]) > self.max_seqlen:
        return None

    return input_ids, label_ids
```

**æ”¹è¿›ç‚¹**ï¼š
- åœ¨ç¼–ç å‰æ£€æŸ¥ `input` å’Œ `label` æ˜¯å¦ä¸º `None`
- æå‰è¿”å› `None`ï¼Œé¿å…åç»­å¤„ç†å¤±è´¥

---

## éƒ¨ç½²æ­¥éª¤

### æ­¥éª¤ 1: ä¸Šä¼ ä¿®å¤åçš„æ–‡ä»¶åˆ°æœåŠ¡å™¨

```bash
# ä» Mac ä¸Šä¼ 
scp /Users/huangziheng/PycharmProjects/all_code/codeH1_4090/code/zh-LAT-peft/mamba-peft/dataset/dart_data.py \
    user@your-server:/home/user/mzs_h/code/zh-LAT-peft/mamba-peft/dataset/

scp /Users/huangziheng/PycharmProjects/all_code/codeH1_4090/code/zh-LAT-peft/mamba-peft/dataset/base.py \
    user@your-server:/home/user/mzs_h/code/zh-LAT-peft/mamba-peft/dataset/
```

### æ­¥éª¤ 2: åœ¨æœåŠ¡å™¨ä¸Šæ¸…ç†ç¼“å­˜

```bash
cd /home/user/mzs_h/code/zh-LAT-peft/mamba-peft

# åˆ é™¤æ‰€æœ‰ DART ç¼“å­˜ï¼ˆè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼‰
rm -fv data/GEM_dart/cache_GEM_dart_*.pkl
rm -rfv data/GEM_dart/parts/

# æ¸…ç† Python ç¼“å­˜
find . -type d -name __pycache__ -exec rm -rf {} + 2>/dev/null || true
find . -name "*.pyc" -delete
```

### æ­¥éª¤ 3: æµ‹è¯•éªŒè¯é›†åŠ è½½

```bash
cd /home/user/mzs_h/code/zh-LAT-peft/mamba-peft

python3 - <<'PY'
import os, sys
sys.path.insert(0, ".")
os.environ["DART_LOCAL_DIR"] = "data/GEM_dart"

from transformers import AutoTokenizer
from dataset.dart_data import DartDataset

tok = AutoTokenizer.from_pretrained(
    "/home/user/mzs_h/model/second-gla-1.3B-100B/gla-1.3B-100B",
    trust_remote_code=True
)

print("æµ‹è¯•éªŒè¯é›†åŠ è½½...")
ds_val = DartDataset(tok, split="val", mode="gen", use_cache=True)
print(f"âœ“ éªŒè¯é›†: {len(ds_val)} æ ·æœ¬")

if len(ds_val) > 0:
    sample = ds_val[0]
    print(f"âœ“ ç¬¬ä¸€ä¸ªæ ·æœ¬:")
    print(f"    input_ids: {sample['input_ids'].shape}")
    print(f"    label_ids: {sample['label_ids'].shape}")
else:
    print("âœ— éªŒè¯é›†ä»ç„¶ä¸ºç©ºï¼")
    sys.exit(1)

print("\nâœ“âœ“âœ“ éªŒè¯é›†åŠ è½½æˆåŠŸï¼")
PY
```

### æ­¥éª¤ 4: é‡å¯è®­ç»ƒ

```bash
cd /home/user/mzs_h/code/zh-LAT-peft/mamba-peft/scripts/train/new

export DART_LOCAL_DIR=/home/user/mzs_h/code/zh-LAT-peft/mamba-peft/data/GEM_dart/

EVAL_GEN=1 \
EVAL_GEN_MAX_LENGTH=128 \
EVAL_GEN_MIN_LENGTH=5 \
EVAL_GEN_NUM_BEAMS=4 \
HP_EVAL_STEPS=3000 \
HP_SAVE_STEPS=3000 \
HP_LOGGING_STEPS=300 \
SWANLAB_ENABLE=1 \
SWANLAB_MODE=cloud \
SWANLAB_PROJECT="gla-mamba-dart-fixed" \
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
TOKENIZERS_PARALLELISM=false \
OMP_NUM_THREADS=1 \
MKL_NUM_THREADS=1 \
NUM_DATA_WORKERS=8 \
PREFETCH_FACTOR=4 \
GRADIENT_CHECKPOINTING=true \
LOGITS_TO_KEEP=1 \
./gla_batch_tmux.sh --suite E5 --round all \
  --pairs "87:dart" \
  --gpus "1 2 3 4 5" \
  --gpu-plan "2,2,2,2,2"
```

---

## éªŒè¯ä¿®å¤

ä¿®å¤åï¼Œåº”è¯¥çœ‹åˆ°ï¼š

1. **ç¼“å­˜ç”Ÿæˆæ—¶**ï¼š
   ```
   Parallel processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2768/2768 [00:05<00:00, 500.00it/s]
   Warning: 10/2768 samples returned None (will be filtered out)
   âœ“ val_gen cache warmed: 2758 samples
   ```

2. **è®­ç»ƒæ—¥å¿—**ï¼š
   ```
   1%|â– | 2000/156650 [12:00<10:30:00, 4.10it/s]
   Evaluate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2758/2758 [05:30<00:00, 8.35it/s]
   {'eval_meteor': 0.35, 'eval_bleu': 0.28, ...}
   ```

3. **ä¸å†å‡ºç°**ï¼š
   ```
   âœ— TypeError: 'NoneType' object is not subscriptable
   âœ— ERROR: All samples were filtered out
   ```

---

## ä¿®å¤æ€»ç»“

| ä¿®å¤ | æ–‡ä»¶ | ä½ç½® | ä½œç”¨ |
|------|------|------|------|
| 1 | `dart_data.py` | 263-289 | å¤„ç†åµŒå¥—åˆ—è¡¨/numpy æ•°ç»„ |
| 2 | `dart_data.py` | 387-429 | æ”¹è¿›é”™è¯¯å¤„ç†ï¼Œè¿”å› None è€Œä¸æ˜¯æŠ›å¼‚å¸¸ |
| 3 | `base.py` | 111-124 | åœ¨ preproc ä¸­å¤„ç† None è¿”å›å€¼ |

**å…³é”®æ”¹è¿›**ï¼š
- âœ… é€’å½’å±•å¹³åµŒå¥—ç»“æ„
- âœ… ä¼˜é›…å¤„ç†æ— æ•ˆæ ·æœ¬ï¼ˆè¿”å› Noneï¼‰
- âœ… é˜²æ­¢ç©ºç¼“å­˜å¯¼è‡´è®­ç»ƒå´©æºƒ
- âœ… è¯¦ç»†çš„è­¦å‘Šä¿¡æ¯ä¾¿äºè°ƒè¯•

---

## å¦‚æœä»ç„¶å¤±è´¥

å¦‚æœä¿®å¤åä»ç„¶å‡ºç°é—®é¢˜ï¼Œè¿è¡Œè¯Šæ–­ï¼š

```bash
cd /home/user/mzs_h/code/zh-LAT-peft/mamba-peft

python3 - <<'PY'
import os, sys
sys.path.insert(0, ".")
os.environ["DART_LOCAL_DIR"] = "data/GEM_dart"

from transformers import AutoTokenizer
from dataset.dart_data import DartDataset

tok = AutoTokenizer.from_pretrained(
    "/home/user/mzs_h/model/second-gla-1.3B-100B/gla-1.3B-100B",
    trust_remote_code=True
)

# æµ‹è¯•å•ä¸ªæ ·æœ¬ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
ds = DartDataset(tok, split="val", mode="gen", use_cache=False, subset_size=1)
df = ds.load_df()

print(f"DataFrame è¡Œæ•°: {len(df)}")
print(f"ç¬¬ä¸€è¡Œ text ç±»å‹: {type(df.iloc[0]['text'])}")
print(f"ç¬¬ä¸€è¡Œ text å€¼: {df.iloc[0]['text']}")

# æµ‹è¯• get_input_label
try:
    input, label = ds.get_input_label(0)
    print(f"âœ“ get_input_label æˆåŠŸ")
    print(f"  input: {input[:100]}")
    print(f"  label: {label[:100]}")
except Exception as e:
    print(f"âœ— get_input_label å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
PY
```

å°†è¾“å‡ºå‘é€ç»™æˆ‘è¿›è¡Œè¿›ä¸€æ­¥è¯Šæ–­ã€‚

---

**ä¿®å¤å®Œæˆï¼ç°åœ¨è®­ç»ƒåº”è¯¥èƒ½å¤Ÿæ­£å¸¸è¿›è¡Œè¯„ä¼°äº†ã€‚** ğŸš€

