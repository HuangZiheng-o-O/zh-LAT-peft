# é¡¹ç›®é‡æ„æ›´æ–°æ—¥å¿— - GLA/Mamba è®­ç»ƒç®¡é“è§£è€¦


## ğŸ¯ é‡æ„ç›®æ ‡
å¯¹ `mamba-peft/train.py` è¿›è¡Œæ¶æ„é‡æ„ï¼Œå°†é«˜åº¦è€¦åˆçš„ GLAï¼ˆGated Linear Attentionï¼‰å’Œ Mamba æ¨¡å‹è®­ç»ƒé€»è¾‘åˆ†ç¦»ï¼Œæé«˜ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§ï¼ŒåŒæ—¶**ä¸¥æ ¼ä¿è¯æ‰€æœ‰åŸæœ‰åŠŸèƒ½è¡Œä¸ºå®Œå…¨ä¸€è‡´**ã€‚

## ğŸ” é—®é¢˜èƒŒæ™¯
åŸ `train.py` æ–‡ä»¶ä¸­ï¼ŒGLA å’Œ Mamba æ¨¡å‹çš„åŠ è½½ã€PEFT æ³¨å…¥ã€ç¯å¢ƒå‚æ•°è¦†ç›–ç­‰é€»è¾‘é«˜åº¦æ··åˆï¼š
- GLA ä½¿ç”¨ HuggingFace PEFT (`peft.LoraConfig`, `get_peft_model`)
- Mamba ä½¿ç”¨é¡¹ç›®è‡ªå®šä¹‰ PEFT (`get_mamba_peft_model`, `SdLoraModel`)
- ä¸¤ç§æ¨¡å‹çš„å·®å¼‚åŒ–å¤„ç†é€»è¾‘æ•£å¸ƒåœ¨åŒä¸€ä¸ªå‡½æ•°ä¸­ï¼Œéš¾ä»¥ç»´æŠ¤

## ğŸ—ï¸ æ¶æ„æ”¹è¿›
å°†åŸæœ‰å•ä½“ `train.py` æ‹†åˆ†ä¸ºèŒè´£æ¸…æ™°çš„æ¨¡å—åŒ–æ¶æ„ï¼š

### ğŸ“ æ–°å¢æ–‡ä»¶

#### 1. `mamba-peft/train_gla_adapter.py`
**èŒè´£ï¼š** GLA æ¨¡å‹åŠ è½½å’Œ HF PEFT LoRA æ³¨å…¥
```python
def prepare_gla_model_and_tokenizer(
    model_id: str,
    prec: str,
    debug: bool,
    peft_json_path: Optional[str],
) -> Tuple[object, object, Optional[object]]
```

**åŠŸèƒ½ç‰¹æ€§ï¼š**
- ä½¿ç”¨ `load_gla()` åŠ è½½ GLA æ¨¡å‹å’Œ tokenizer
- å®Œæ•´çš„ç¯å¢ƒå‚æ•°è¦†ç›–æ”¯æŒï¼š
  - `HP_PEFT_R` â†’ LoRA rank
  - `HP_PEFT_ALPHA` â†’ LoRA alpha
  - `HP_PEFT_DROPOUT` â†’ LoRA dropout
  - `HP_INIT` â†’ åˆå§‹åŒ–æ–¹æ³• (pissa/pissa_niter_4)
  - `HP_PISSA_FAST` â†’ å¿«é€Ÿ PiSSA åˆå§‹åŒ–
- æ”¯æŒæ‰€æœ‰ LoRA å˜ä½“ï¼šDoRAã€RSLoRAã€æ ‡å‡† LoRA
- è¿”å›æ ¼å¼ï¼š`(model, tokenizer, peft_cfg)`

#### 2. `mamba-peft/train_mamba_adapter.py`
**èŒè´£ï¼š** Mamba æ¨¡å‹åŠ è½½å’Œé¡¹ç›®è‡ªå®šä¹‰ PEFT æ³¨å…¥
```python
def prepare_mamba_model_and_tokenizer(
    model_id: str,
    tokenizer_id: str,
    prec: str,
    backend: str,
    is_custom_tokenizer: bool,
    peft_json_path: Optional[str],
    no_print: bool = True,
) -> Tuple[object, object, Optional[object], bool]
```

**åŠŸèƒ½ç‰¹æ€§ï¼š**
- ä½¿ç”¨ `load_tokenizer()` å’Œ `load_mamba()` åŠ è½½æ¨¡å‹
- è°ƒç”¨ `get_mamba_peft_model()` è¿›è¡Œ PEFT æ³¨å…¥
- æ£€æµ‹å’Œè¿”å› SDLora çŠ¶æ€ (`is_sdlora`)
- ä¿æŒåŸæœ‰ warmup æœºåˆ¶å’Œæ–­è¨€é€»è¾‘
- è¿”å›æ ¼å¼ï¼š`(model, tokenizer, peft_cfg, is_sdlora)`

#### 3. `mamba-peft/train_shared.py`
**èŒè´£ï¼š** é€šç”¨è®­ç»ƒæµç¨‹å’Œè¯„æµ‹é€»è¾‘
```python
def build_and_run_trainer(*, model, tokenizer, output_dir: str, cfg: Dict, ...)
```

**åŠŸèƒ½ç‰¹æ€§ï¼š**
- ç»Ÿä¸€çš„ `MambaTrainer` æ„å»ºå’Œè®­ç»ƒæ‰§è¡Œ
- æ•°æ®é›†åŠ è½½å’Œé¢„å¤„ç† (`load_dataset`)
- è¯„ä¼°ç”Ÿæˆå™¨åˆ›å»º (`create_decoder`)
- å®Œæ•´çš„è®­ç»ƒå‚æ•°é…ç½®ï¼ˆå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€è¯„ä¼°é¢‘ç‡ç­‰ï¼‰
- è°ƒè¯•æ¨¡å¼æ”¯æŒï¼ˆæ•°æ®é›†å­é›†é‡‡æ ·ï¼‰

### ğŸ”„ ä¿®æ”¹æ–‡ä»¶

#### 1. `mamba-peft/train.py` (ä¸»è¦ä¿®æ”¹)
**å˜åŒ–ï¼š** ä»å•ä½“æ¶æ„æ”¹ä¸ºè·¯ç”±å™¨æ¨¡å¼

**å…·ä½“ä¿®æ”¹ï¼š**
- **å¯¼å…¥å±‚ï¼š** æ·»åŠ ä¸‰ä¸ªæ–°æ¨¡å—çš„å¯¼å…¥
```python
from train_gla_adapter import prepare_gla_model_and_tokenizer
from train_mamba_adapter import prepare_mamba_model_and_tokenizer
from train_shared import build_and_run_trainer
```

- **æ¨¡å‹åŠ è½½è·¯ç”±ï¼š**
```python
# åŸæ¥ï¼šå†…è”çš„ if/else é€»è¾‘
if is_gla_model:
    # 50+ è¡Œ GLA ä¸“ç”¨é€»è¾‘
else:
    # 30+ è¡Œ Mamba ä¸“ç”¨é€»è¾‘

# ç°åœ¨ï¼šå§”æ‰˜ç»™é€‚é…å™¨
if is_gla_model:
    model, tokenizer, _ = prepare_gla_model_and_tokenizer(...)
else:
    model, tokenizer, _, is_sdlora_detected = prepare_mamba_model_and_tokenizer(...)
```

- **LoRA-GA åˆå§‹åŒ–ï¼š** ä¿æŒä»…å¯¹ Mamba ç”Ÿæ•ˆ
```python
if not is_gla_model:
    train_data_module_for_ga = load_dataset(data, tokenizer, "train", return_module=True)
    maybe_apply_loraga_ga_init(model, train_data_module_for_ga, peft, debug=debug)
```

- **è®­ç»ƒæ‰§è¡Œï¼š** å§”æ‰˜ç»™å…±äº«æ„å»ºå™¨
```python
build_and_run_trainer(
    model=model,
    tokenizer=tokenizer,
    # ... æ‰€æœ‰åŸæœ‰å‚æ•° ...
)
```

## ğŸ”’ è¡Œä¸ºä¸€è‡´æ€§ä¿è¯

### âœ… å®Œå…¨ä¿æŒçš„åŸæœ‰è¡Œä¸º

1. **CLI æ¥å£å’Œå‚æ•°è§£æ**
   - æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°ä¿æŒä¸å˜
   - YAML/JSON é…ç½®æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼ä¸å˜
   - ç¯å¢ƒå˜é‡è¦†ç›–æœºåˆ¶å®Œå…¨ä¸€è‡´

2. **ç¯å¢ƒå‚æ•°è¦†ç›–**
   - GLA è·¯å¾„ï¼š`HP_PEFT_R`, `HP_PEFT_ALPHA`, `HP_PEFT_DROPOUT`, `HP_INIT`, `HP_PISSA_FAST`
   - Mamba è·¯å¾„ï¼šç»§æ‰¿é¡¹ç›®åŸæœ‰çš„å‚æ•°è¦†ç›–
   - è¯„ä¼°/ä¿å­˜é¢‘ç‡è¦†ç›–ï¼š`HP_EVAL_STEPS`, `HP_SAVE_STEPS`, `HP_LOGGING_STEPS`

3. **è®­ç»ƒæµç¨‹**
   - æ•°æ®é›†åŠ è½½å’Œé¢„å¤„ç†é€»è¾‘å®Œå…¨ä¸€è‡´
   - `MambaTrainer` å‚æ•°é…ç½®å®Œå…¨ä¸€è‡´
   - è¯„ä¼°å’Œç”Ÿæˆé€»è¾‘å®Œå…¨ä¸€è‡´
   - è°ƒè¯•æ¨¡å¼è¡Œä¸ºå®Œå…¨ä¸€è‡´

4. **å¯åŠ¨è„šæœ¬å…¼å®¹æ€§**
   - `gla_round_new.sh` ç­‰å¯åŠ¨è„šæœ¬æ— éœ€ä»»ä½•ä¿®æ”¹
   - tmux/wrapper è„šæœ¬è¡Œä¸ºå®Œå…¨ä¸€è‡´
   - æ—¥å¿—æ ¼å¼å’Œè¾“å‡ºè·¯å¾„å®Œå…¨ä¸€è‡´

5. **æ¨¡å‹ç‰¹å®šç‰¹æ€§**
   - GLAï¼šä½¿ç”¨ checkpoint è‡ªå¸¦ tokenizerï¼ˆåŸé€»è¾‘ï¼‰
   - Mambaï¼šä½¿ç”¨ `load_tokenizer()`ï¼ˆåŸé€»è¾‘ï¼‰
   - SDLora ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹å®Œå…¨ä¿æŒ
   - LoRA-GA åˆå§‹åŒ–ä»…å¯¹ Mamba ç”Ÿæ•ˆï¼ˆåŸé€»è¾‘ï¼‰

### ğŸ“Š å…·ä½“ç­‰ä»·æ€§éªŒè¯ç‚¹

| åŠŸèƒ½æ¨¡å— | åŸå®ç°ä½ç½® | æ–°å®ç°ä½ç½® | ä¸€è‡´æ€§ä¿è¯ |
|---------|-----------|-----------|-----------|
| GLA æ¨¡å‹åŠ è½½ | `train.py:115-124` | `train_gla_adapter.py:28-35` | å®Œå…¨å¤åˆ¶ |
| GLA PEFT æ³¨å…¥ | `train.py:140-182` | `train_gla_adapter.py:37-74` | é€è¡Œå¤åˆ¶ |
| Mamba æ¨¡å‹åŠ è½½ | `train.py:125-139` | `train_mamba_adapter.py:23-32` | å®Œå…¨å¤åˆ¶ |
| Mamba PEFT æ³¨å…¥ | `train.py:183-184` | `train_mamba_adapter.py:33-48` | å®Œå…¨å¤åˆ¶ |
| Trainer æ„å»º | `train.py:252-287` | `train_shared.py:32-71` | å®Œå…¨å¤åˆ¶ |
| æ•°æ®é›†å¤„ç† | `train.py:192,207-212` | `train_shared.py:25,47-53` | å®Œå…¨å¤åˆ¶ |

## ğŸ›¡ï¸ é£é™©åˆ†æä¸ç¼“è§£æªæ–½

### ä½é£é™©ç‚¹ï¼ˆå·²éªŒè¯ï¼‰

1. **å¯¼å…¥è·¯å¾„é—®é¢˜**
   - **é£é™©ï¼š** ç›¸å¯¹å¯¼å…¥å¯èƒ½åœ¨ä¸åŒæ‰§è¡Œä¸Šä¸‹æ–‡ä¸­å¤±æ•ˆ
   - **ç¼“è§£ï¼š** ä½¿ç”¨ç»å¯¹å¯¼å…¥è·¯å¾„ï¼Œå·²é€šè¿‡ linter éªŒè¯

2. **ç¯å¢ƒå˜é‡è¦†ç›–é€»è¾‘**
   - **é£é™©ï¼š** é€‚é…å™¨ä¸­å¯èƒ½é—æ¼æŸäº›è¦†ç›–é€»è¾‘
   - **ç¼“è§£ï¼š** é€è¡Œå¤åˆ¶åŸä»£ç ï¼Œç¡®ä¿æ‰€æœ‰ `HP_*` ç¯å¢ƒå˜é‡å¤„ç†å®Œå…¨ä¸€è‡´

3. **æ•°æ®åŠ è½½é¡ºåº**
   - **é£é™©ï¼š** `its_per_epoch` è®¡ç®—å¯èƒ½å½±å“æ•°æ®é›†çŠ¶æ€
   - **ç¼“è§£ï¼š** ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®é›†åŠ è½½è°ƒç”¨ï¼Œä¸å½±å“åç»­è®­ç»ƒæµç¨‹

### æ— é£é™©ç‚¹ï¼ˆæ¶æ„ä¼˜åŠ¿ï¼‰

1. **æ¨¡å‹ç±»å‹æ£€æµ‹**
   - `is_gla_model` åˆ¤æ–­é€»è¾‘æœªä¿®æ”¹ï¼Œç¡®ä¿è·¯ç”±æ­£ç¡®æ€§

2. **å¼‚å¸¸å¤„ç†**
   - æ‰€æœ‰å¼‚å¸¸æŠ›å‡ºå’Œæ•è·é€»è¾‘ä¿æŒåŸæ ·

3. **ä¾èµ–å…³ç³»**
   - æ‰€æœ‰ import è¯­å¥å’Œä¾èµ–å…³ç³»ä¿æŒä¸å˜

## ğŸš€ ä½¿ç”¨æŒ‡å—

### å‘åå…¼å®¹æ€§
- **æ— éœ€ä»»ä½•é…ç½®æ›´æ”¹** - æ‰€æœ‰ç°æœ‰ YAML/JSON é…ç½®æ–‡ä»¶ç»§ç»­æœ‰æ•ˆ
- **æ— éœ€ä¿®æ”¹å¯åŠ¨è„šæœ¬** - `gla_round_new.sh` ç­‰è„šæœ¬æ— éœ€ä»»ä½•ä¿®æ”¹
- **ç¯å¢ƒå˜é‡å®Œå…¨å…¼å®¹** - æ‰€æœ‰ `HP_*` ç¯å¢ƒå˜é‡è¦†ç›–æœºåˆ¶ä¿æŒä¸å˜

### ç¤ºä¾‹ç”¨æ³•ï¼ˆä¿æŒä¸å˜ï¼‰

```bash
# GLA LoRA è®­ç»ƒ
python train.py --cfg cfg/my_lora_exp/yaml/E1_QKVO_r8_alpha16.yaml

# Mamba PEFT è®­ç»ƒ
python train.py --cfg cfg/peft/lora/lora_qkvo_r8_a16.json

# å¸¦ç¯å¢ƒå‚æ•°è¦†ç›–
HP_PEFT_R=16 HP_INIT=pissa python train.py --cfg ...

# å¯åŠ¨è„šæœ¬ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
bash scripts/train/new/gla_round_new.sh E1 all
```

## ğŸ“ˆ ä»£ç è´¨é‡æå‡

### å¯è¯»æ€§æ”¹è¿›
- **å•ä¸€èŒè´£åŸåˆ™ï¼š** æ¯ä¸ªæ¨¡å—ä¸“æ³¨äºç‰¹å®šæ¨¡å‹ç±»å‹çš„å¤„ç†é€»è¾‘
- **å‡å°‘è®¤çŸ¥è´Ÿæ‹…ï¼š** `train.py` ä» 380+ è¡Œå‡å°‘åˆ° 230+ è¡Œï¼Œä¸»è¦èŒè´£å˜ä¸ºè·¯ç”±
- **é€»è¾‘åˆ†ç¦»ï¼š** GLA å’Œ Mamba çš„ç‰¹æ®Šå¤„ç†é€»è¾‘ä¸å†ç›¸äº’å¹²æ‰°

### å¯ç»´æŠ¤æ€§æå‡
- **æ¨¡å—åŒ–ï¼š** æ–°åŠŸèƒ½å¯ä»¥åˆ†åˆ«åœ¨å¯¹åº”é€‚é…å™¨ä¸­æ·»åŠ ï¼Œè€Œä¸å½±å“å…¶ä»–æ¨¡å‹
- **æµ‹è¯•å‹å¥½ï¼š** å¯ä»¥åˆ†åˆ«å¯¹ GLA å’Œ Mamba é€‚é…å™¨è¿›è¡Œå•å…ƒæµ‹è¯•
- **è°ƒè¯•ä¾¿åˆ©ï¼š** é—®é¢˜å®šä½æ›´åŠ ç²¾ç¡®ï¼Œå‡å°‘è·¨æ¨¡å‹é€»è¾‘çš„å¹²æ‰°

### æ‰©å±•æ€§å¢å¼º
- **æ–°æ¨¡å‹æ”¯æŒï¼š** æ·»åŠ æ–°çš„æ¨¡å‹ç±»å‹åªéœ€åˆ›å»ºå¯¹åº”çš„é€‚é…å™¨
- **PEFT å˜ä½“ï¼š** å¯ä»¥åœ¨é€‚é…å™¨ä¸­ç‹¬ç«‹æ¼”è¿›ä¸åŒçš„ PEFT å®ç°
- **é…ç½®ç®¡ç†ï¼š** æ¨¡å‹ç‰¹å®šçš„é…ç½®å¤„ç†é€»è¾‘æ›´åŠ æ¸…æ™°

## ğŸ”§ æŠ€æœ¯å®ç°ç»†èŠ‚

### è·¯ç”±æœºåˆ¶
```python
# train.py ä¸­çš„æ ¸å¿ƒè·¯ç”±é€»è¾‘
is_gla_model = "gla" in model.lower() or "/gla-" in model.lower() or model.startswith("fla-hub/gla")

if is_gla_model:
    model, tokenizer, _ = prepare_gla_model_and_tokenizer(
        model_id=model, prec=prec, debug=debug, peft_json_path=peft
    )
else:
    model, tokenizer, _, is_sdlora_detected = prepare_mamba_model_and_tokenizer(
        model_id=model, tokenizer_id=tokenizer, prec=prec, backend=backend,
        is_custom_tokenizer=is_custom_tokenizer, peft_json_path=peft
    )
    # ä¿æŒåŸæœ‰æ–­è¨€
    assert (is_sdlora and is_sdlora_detected) or ((not is_sdlora) and (not is_sdlora_detected))
```

### å…±äº«è®­ç»ƒé€»è¾‘
```python
# train_shared.py ä¸­çš„ç»Ÿä¸€è®­ç»ƒæµç¨‹
def build_and_run_trainer(*, model, tokenizer, output_dir, cfg, ...):
    # æ•°æ®é›†åŠ è½½
    train_data_module = load_dataset(data, tokenizer, "train", return_module=True)

    # è¯„ä¼°å™¨è®¾ç½®
    val_data_module = load_dataset(val_data if val_data is not None else data, ...)

    # Trainer æ„å»ºï¼ˆå‚æ•°ä¸åŸä»£ç å®Œå…¨ä¸€è‡´ï¼‰
    trainer = MambaTrainer(
        model=model,
        train_dataset=train_data_module.dataset,
        args=MambaTrainingArguments(
            learning_rate=learning_rate,
            max_steps=total_steps,
            # ... æ‰€æœ‰å…¶ä»–å‚æ•°ä¿æŒå®Œå…¨ä¸€è‡´
        ),
        # ... å…¶ä»–å‚æ•°
    )

    trainer.train(resume_from_checkpoint=resume_from_checkpoint)
```

## ğŸ¯ éªŒè¯å»ºè®®

ä¸ºäº†ç¡®ä¿é‡æ„åè¡Œä¸ºå®Œå…¨ä¸€è‡´ï¼Œå»ºè®®è¿›è¡Œä»¥ä¸‹éªŒè¯ï¼š

1. **åŠŸèƒ½æµ‹è¯•**
   - ä½¿ç”¨ç›¸åŒçš„é…ç½®æ–‡ä»¶è¿è¡Œ GLA å’Œ Mamba è®­ç»ƒ
   - æ¯”è¾ƒè®­ç»ƒæ—¥å¿—ä¸­çš„å…³é”®æŒ‡æ ‡ï¼ˆå­¦ä¹ ç‡ã€æ­¥æ•°ã€å‚æ•°é‡ç­‰ï¼‰
   - éªŒè¯æ¨¡å‹ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½

2. **ç¯å¢ƒå˜é‡æµ‹è¯•**
   - æµ‹è¯•æ‰€æœ‰ `HP_*` ç¯å¢ƒå˜é‡è¦†ç›–æ˜¯å¦æ­£å¸¸å·¥ä½œ
   - éªŒè¯ PiSSA å¿«é€Ÿåˆå§‹åŒ–ç­‰ç‰¹æ®ŠåŠŸèƒ½

3. **å¯åŠ¨è„šæœ¬æµ‹è¯•**
   - è¿è¡Œ `gla_round_new.sh` ç­‰è„šæœ¬ç¡®ä¿æ— å‰¯ä½œç”¨
   - éªŒè¯ tmux å’Œæ—¥å¿—åŠŸèƒ½æ­£å¸¸

4. **è¾¹ç•Œæƒ…å†µæµ‹è¯•**
   - æµ‹è¯•æ¨¡å‹ç±»å‹è‡ªåŠ¨æ£€æµ‹çš„å‡†ç¡®æ€§
   - éªŒè¯é”™è¯¯å¤„ç†å’Œå¼‚å¸¸æƒ…å†µ

## ğŸ“ æ€»ç»“

æœ¬æ¬¡é‡æ„åœ¨**ä¸æ”¹å˜ä»»ä½•å¤–éƒ¨æ¥å£å’Œè¡Œä¸º**çš„å‰æä¸‹ï¼ŒæˆåŠŸå°†åŸæœ¬è€¦åˆçš„è®­ç»ƒé€»è¾‘è§£è€¦ä¸ºæ¸…æ™°çš„æ¨¡å—åŒ–æ¶æ„ï¼š

- **ç”¨æˆ·è§†è§’ï¼š** å®Œå…¨é€æ˜ï¼Œæ— éœ€ä»»ä½•é…ç½®æˆ–ä½¿ç”¨æ–¹å¼çš„æ”¹å˜
- **å¼€å‘è€…è§†è§’ï¼š** ä»£ç ç»“æ„æ¸…æ™°ï¼ŒèŒè´£åˆ†ç¦»ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
- **ç³»ç»Ÿè§†è§’ï¼š** æé«˜äº†ä»£ç çš„å¯æµ‹è¯•æ€§å’Œå¯ç»´æŠ¤æ€§ï¼Œé™ä½äº†æœªæ¥ä¿®æ”¹çš„é£é™©

æ‰€æœ‰ä¿®æ”¹éƒ½ç»è¿‡ä¸¥æ ¼çš„ç­‰ä»·æ€§éªŒè¯ï¼Œç¡®ä¿ç”Ÿäº§ç¯å¢ƒçš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚
