
# Bugfix Log: ä¿®å¤ Spider è¯„æµ‹ `'select' not found` å´©æºƒ & ç´¢å¼•é”™ä½éšæ‚£

## ä¸€ã€é—®é¢˜èƒŒæ™¯

åœ¨ Spider Text-to-SQL ä»»åŠ¡ä¸Šè®­ç»ƒ/è¯„ä¼°æ—¶ï¼Œ`trainer.evaluate()` é˜¶æ®µå‘¨æœŸæ€§è§¦å‘ SQL metric è®¡ç®—ã€‚  
åœ¨è¿™ä¸ªé˜¶æ®µï¼Œç¨‹åºåœ¨ Spider å®˜æ–¹çš„ SQL è§£æè„šæœ¬ä¸­å´©æºƒï¼Œæ—¥å¿—ç±»ä¼¼ï¼š

```text
Computing SQL metrics...:   0%|          | 0/1375 [00:00<?, ?it/s]
...
File "metrics/spider/process_sql.py", line 345, in parse_select
    assert toks[idx] == 'select', "'select' not found"
AssertionError: 'select' not found
````

å †æ ˆå¯è¿½è¸ªåˆ°ï¼š

* `SpiderDataset.compute_metrics(...)` è°ƒç”¨ `SpiderMetric.compute(...)`
* `SpiderMetric.compute(...)` å†è°ƒç”¨ Spider å®˜æ–¹ `evaluate(...)`
* `evaluate(...)` å†…éƒ¨å°†å­—ç¬¦ä¸² SQL è§£æä¸º ASTï¼Œè¦æ±‚ SQL ä»¥ `select` å¼€å¤´

---

## äºŒã€æ ¹å› åˆ†æ

### 1. ä½¿ç”¨äº†â€œè§£ç åçš„ label æ–‡æœ¬â€å½“ä½œ reference SQL

åŸå§‹é€»è¾‘ä¸­ï¼Œ`SpiderDataset.compute_metrics` æ„é€  metrics è¾“å…¥æ—¶ï¼š

```python
db_ids = [self.get_db_id(i) for i in range(len(self))]
assert len(db_ids) == len(eval_preds.preds)
assert len(db_ids) == len(eval_preds.labels)

predictions = list(zip(eval_preds.preds, db_ids))
references = list(zip(eval_preds.labels, db_ids))
```

è¿™é‡Œçš„ `eval_preds.labels` æ˜¯ **ä» token IDs decode å›æ¥çš„æ–‡æœ¬**ï¼Œè€Œä¸æ˜¯æ•°æ®é›†ä¸­åŸå§‹çš„ `query` å­—æ®µã€‚
åœ¨è®­ç»ƒ/ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œlabel é€šå¸¸ä¼šå¥—åœ¨ prompt æ¨¡æ¿åé¢ï¼Œå¯èƒ½åŒ…å«ï¼š

* instruction å‰ç¼€ï¼ˆå¦‚ `[INST] ... [/INST]`ï¼‰
* ç‰¹æ®Š tokenï¼ˆå¦‚ `<s>`ã€`</s>`ï¼‰
* å…¶ä»–é SQL çš„ä¸Šä¸‹æ–‡æ–‡æœ¬

å½“è¿™äº›â€œå‰ç¼€åƒåœ¾â€å‡ºç°åœ¨å­—ç¬¦ä¸²å¼€å¤´æ—¶ï¼ŒSpider çš„ parser çœ‹åˆ°çš„ç¬¬ä¸€ä¸ª token å°±**ä¸æ˜¯** `select`ï¼Œäºæ˜¯è§¦å‘ï¼š

```python
assert toks[idx] == 'select', "'select' not found"
```

ğŸ‘‰ **ç›´æ¥å¯¼è‡´è¯„ä¼°é˜¶æ®µå´©æºƒ**ã€‚

---

### 2. æ½œåœ¨éšæ‚£ï¼šHF Dataset ç´¢å¼•ä¸ `self.data` ç´¢å¼•é”™ä½

`SpiderDataset` ç»§æ‰¿è‡ª `NlgDatasetBase`ï¼Œæ•°æ®æ„å»ºè·¯å¾„å¤§è‡´ä¸ºï¼š

* åŸå§‹ HF Datasetï¼š`self.hf_dataset[0]`
* é€šè¿‡ `preproc` æ„é€ å®é™…ç”¨äºè®­ç»ƒ/è¯„ä¼°çš„ `self.data`ï¼Œç»“æ„ç±»ä¼¼ï¼š

  ```python
  self.data[i] = (inputs_labels, meta)
  ```

åŸæ¥çš„ `preproc`ï¼š

```python
def preproc(self, idx):
    inputs_labels = super().preproc(idx)
    if inputs_labels is None:
        return None
    return inputs_labels, {"db_id": self.hf_dataset[0]["db_id"][idx]}
```

è¿™æ˜¯ä¸€ä¸ªå…³é”®ç‚¹ï¼š

* `super().preproc(idx)` å¯èƒ½è¿”å› `None` â†’ è¯¥æ ·æœ¬è¢«è¿‡æ»¤ï¼ˆå¤ªé•¿ç­‰ï¼‰
* åªè¦è¿”å› `None`ï¼Œè¯¥æ ·æœ¬å°±ä¸ä¼šå†™å…¥ `self.data`

ç„¶è€Œæ—§çš„ metric ä»£ç é‡Œï¼š

```python
db_ids   = [self.get_db_id(i) for i in range(len(self))]  # åŸºäº self.data
labels   = eval_preds.labels                               # Trainer åŸºäº self.data éå†å‡ºçš„é¢„æµ‹/label
# ä½†å¦‚æœä» hf_dataset[0] å†å»æŒ‰ i è®¿é—® queryï¼Œå°±ä¼šå‡è®¾ "i == åŸå§‹ idx"ï¼Œè¿™æ˜¯ä¸å¯é çš„
```

ä¸€æ—¦æœ‰æ ·æœ¬åœ¨ `preproc` é˜¶æ®µè¢«ä¸¢å¼ƒï¼Œ**åŸå§‹ HF Dataset çš„ç´¢å¼•å°±ä¸å†ä¸ `self.data` ä¸€ä¸€å¯¹åº”**ã€‚
è¿™ä¼šå¯¼è‡´ï¼š

* prediction / reference / db_id ä¹‹é—´å‡ºç°é™é»˜é”™ä½ï¼›
* å³ä½¿ä¸å´©æºƒï¼Œè¯„ä¼°æŒ‡æ ‡ä¹Ÿä¼šä¸¥é‡å¤±çœŸã€‚

---

## ä¸‰ã€ä¿®å¤æ€è·¯æ¦‚è§ˆ

ç›®æ ‡æœ‰ä¸‰ï¼š

1. **å½»åº•ä¸å†ä¾èµ– decoded label æ–‡æœ¬ä½œä¸º reference SQL**
2. **ä»æºå¤´ä¿è¯ prediction / reference / db_id ä¸‰è€…ç´¢å¼•å®Œç¾å¯¹é½**
3. **å…¼å®¹å·²æœ‰æ•°æ® cacheï¼Œä½†é¿å…â€œé™é»˜é”™ä½â€â€”â€”æ—§ cache ä¸€å¾‹æ˜¾å¼æŠ¥é”™æç¤ºæ¸…ç†**

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹ `SpiderDataset` åšäº†ä¸¤ç±»ä¿®æ”¹ï¼š

1. åœ¨ `preproc` é˜¶æ®µï¼Œå°† ground-truth SQL å†™å…¥ per-sample metadataï¼š

   * `meta = {"db_id": ..., "query": canonical_sql}`
2. åœ¨ `compute_metrics` ä¸­ï¼Œä»…ä½¿ç”¨ `self.data` ä¸­çš„ `db_id` ä¸ `query`ï¼š

   * ä¸å†ä¾èµ– `hf_dataset[0][i]` / `eval_preds.labels`

---

## å››ã€å…·ä½“ä»£ç æ”¹åŠ¨

### 1. `get_input_label`ï¼šæ›´æ¸…æ™°åœ°ä½¿ç”¨ HF Dataset

**åŸå§‹å®ç°ï¼š**

```python
def get_input_label(self, idx):
    self.get_hf_dataset()

    question = self.hf_dataset[0]["question"][idx]
    db_id = self.hf_dataset[0]["db_id"][idx]
    query = self.hf_dataset[0]["query"][idx]

    table = self.hf_dataset[1][db_id]

    input = f"Question: {question}\nSchema: {table}\n"
    label = query.lower().strip()
        
    return input, label
```

**æ–°å®ç°ï¼š**

```python
def get_input_label(self, idx):
    hf_ds, _ = self.get_hf_dataset()

    question = hf_ds["question"][idx]
    db_id = hf_ds["db_id"][idx]
    query = hf_ds["query"][idx]

    table = self.hf_dataset[1][db_id]

    input = f"Question: {question}\nSchema: {table}\n"
    label = query.lower().strip()
        
    return input, label
```

**å˜åŒ–ç‚¹ï¼š**

* ä½¿ç”¨è§£æ„ `hf_ds, _ = self.get_hf_dataset()`ï¼Œä½¿å¾— `hf_ds` è¯­ä¹‰æ›´æ¸…æ™°ï¼ˆHF Dataset æœ¬ä½“ï¼‰ï¼Œé¿å…åˆ°å¤„å†™ `self.hf_dataset[0]`ã€‚
* è¡Œä¸ºä¸Šä¸å˜ï¼š`label` ä»ç„¶æ˜¯ `query.lower().strip()`ã€‚

> è¿™ä¸€æ”¹åŠ¨ä¸»è¦æ˜¯å¯è¯»æ€§æå‡ï¼Œä¸ºåç»­é€»è¾‘ç»Ÿä¸€æ‰“åŸºç¡€ã€‚

---

### 2. `preproc`ï¼šå°† canonical SQL å†™å…¥ metadataï¼ˆå…³é”®æ”¹åŠ¨ï¼‰

**åŸå§‹å®ç°ï¼š**

```python
def preproc(self, idx):
    inputs_labels = super().preproc(idx)

    if inputs_labels is None:
        return None

    return inputs_labels, {"db_id": self.hf_dataset[0]["db_id"][idx]}
```

**æ–°å®ç°ï¼š**

```python
def preproc(self, idx):
    """
    Build one training/eval example.
    We:
    - Use the base class to create (input_ids, label_ids)
    - Attach metadata with db_id and canonical SQL query (lower+strip) so that
      generation metrics can safely use the ground-truth SQL without relying
      on decoded labels or fragile index assumptions.
    """
    inputs_labels = super().preproc(idx)

    if inputs_labels is None:
        return None

    hf_ds, _ = self.get_hf_dataset()
    meta = {
        "db_id": hf_ds["db_id"][idx],
        "query": str(hf_ds["query"][idx]).lower().strip(),
    }
    return inputs_labels, meta
```

**æ ¸å¿ƒå˜åŒ–ï¼š**

* `meta` ä¸­æ–°å¢äº† `"query"` å­—æ®µï¼ˆcanonical SQLï¼š`lower().strip()`ï¼‰ã€‚
* ç”±äº `preproc` åªåœ¨æ ·æœ¬ä¿ç•™æ—¶æ‰è¿”å› `(inputs_labels, meta)`ï¼Œ
  ğŸ‘‰ **`self.data[i]` ä¸­çš„ `meta` ä¸ Trainer å®é™…ä½¿ç”¨çš„æ ·æœ¬å®Œå…¨åŒæ­¥**ã€‚

è¿™ä¸ºåç»­ metrics ä½¿ç”¨ `self.data[i][1]["query"]` ä½œä¸º reference SQL å¥ å®šäº†åŸºç¡€ã€‚

---

### 3. `compute_metrics`ï¼šä»…ä» `self.data` è¯» metadataï¼Œå½»åº•ä¸¢å¼ƒ decoded labels

**åŸå§‹å®ç°ï¼š**

```python
def compute_metrics(self, eval_preds, eval_mask=None):
    if self.mode == "gen":
        metric = SpiderMetric()

        db_ids = [self.get_db_id(i) for i in range(len(self))]
        assert len(db_ids) == len(eval_preds.preds)
        assert len(db_ids) == len(eval_preds.labels)

        predictions = list(zip(eval_preds.preds, db_ids))
        references = list(zip(eval_preds.labels, db_ids))

        if eval_mask is not None:
            predictions = [predictions[i] for i in eval_mask]
            references = [references[i] for i in eval_mask]

        metrics = metric.compute(predictions, references)

        # important metric first
        return {
            "all/exec": None,
            **metrics
        }
    else:
        return {}
```

**æ–°å®ç°ï¼š**

```python
def compute_metrics(self, eval_preds, eval_mask=None):
    if self.mode == "gen":
        # Ensure dataset and HF view are initialized (handles lazy reload cases)
        if self.data is None:
            # Import from base without circular import
            from .base import DatasetBase  # type: ignore
            DatasetBase._ensure_materialized(self)  # type: ignore[attr-defined]

        metric = SpiderMetric()

        size = len(self)
        # Guard against legacy caches that don't carry 'query' in metadata
        sample_meta = self.data[0][1] if (self.data and len(self.data) > 0) else {}
        if "query" not in sample_meta:
            raise RuntimeError(
                "SpiderDataset.compute_metrics expected per-sample metadata with 'query', "
                "but current cache is missing it. Please clear the Spider cache directory "
                "(e.g., data/xlangai_spider_*/cache_*.pkl) or set DATA_CACHE_TAG to a new "
                "value and rerun so the dataset can be rebuilt."
            )

        # For each in-memory example, use the attached db_id + canonical SQL query.
        db_ids = [self.data[i][1]["db_id"] for i in range(size)]
        gt_queries = [self.data[i][1]["query"] for i in range(size)]

        assert len(db_ids) == len(eval_preds.preds) == len(gt_queries)

        predictions = list(zip(eval_preds.preds, db_ids))
        references = list(zip(gt_queries, db_ids))

        if eval_mask is not None:
            predictions = [predictions[i] for i in eval_mask]
            references = [references[i] for i in eval_mask]

        metrics = metric.compute(predictions, references)

        # important metric first
        return {
            "all/exec": None,
            **metrics
        }
    else:
        return {}
```

**å…³é”®ç‚¹è¯´æ˜ï¼š**

1. **Lazy materialization é˜²å¾¡ï¼š**

   ```python
   if self.data is None:
       from .base import DatasetBase
       DatasetBase._ensure_materialized(self)
   ```

   * ç¡®ä¿åœ¨ `compute_metrics` æ—¶ï¼Œ`self.data` å·²ç»æ„å»ºå®Œæˆã€‚
   * é€‚é…æŸäº› lazy åœºæ™¯ï¼šè¯„ä¼°å¯èƒ½å‘ç”Ÿåœ¨é¦–æ¬¡ materialize ä¹‹å‰ã€‚

2. **æ—§ cache æ˜¾å¼æŠ¥é”™ï¼ˆé˜²æ­¢é™é»˜é”™ä½ï¼‰ï¼š**

   ```python
   sample_meta = self.data[0][1] if (self.data and len(self.data) > 0) else {}
   if "query" not in sample_meta:
       raise RuntimeError(
           "SpiderDataset.compute_metrics expected per-sample metadata with 'query', "
           "but current cache is missing it. Please clear the Spider cache directory "
           "(e.g., data/xlangai_spider_*/cache_*.pkl) or set DATA_CACHE_TAG to a new "
           "value and rerun so the dataset can be rebuilt."
       )
   ```

   * æ—§ç‰ˆæœ¬ cache ä¸­çš„ metadata åªæœ‰ `db_id`ï¼Œæ²¡æœ‰ `query`ã€‚
   * ä¸ºé¿å…â€œæ‚„æ‚„ç”¨é”™ referenceâ€ï¼Œç»Ÿä¸€åœ¨ç¬¬ä¸€æ¬¡ eval æ—¶æŠ›å‡ºæ˜ç¡®é”™è¯¯ï¼Œæç¤ºæ¸…ç†/åˆ·æ–° cacheã€‚

3. **å¯¹é½ä¿è¯ï¼šä¸‰ä¸ªå‘é‡éƒ½æ¥è‡ª `self.data` çš„åŒä¸€ç´¢å¼•ç©ºé—´**

   ```python
   db_ids     = [self.data[i][1]["db_id"]  for i in range(size)]
   gt_queries = [self.data[i][1]["query"]  for i in range(size)]

   assert len(db_ids) == len(eval_preds.preds) == len(gt_queries)
   ```

   * `eval_preds.preds` æ˜¯ Trainer é¡ºåºéå† `self.data` çš„è¾“å‡ºï¼›
   * `db_ids` / `gt_queries` ä¹Ÿæ˜¯æŒ‰ `self.data[i]` æ„é€ ï¼›
   * ä¸‰è€…é•¿åº¦ä¸€è‡´ â†’ **ç´¢å¼•ä¸¥æ ¼å¯¹é½**ã€‚

4. **å½»åº•ä¸¢å¼ƒ decoded label ä½œä¸º referenceï¼š**

   ```python
   predictions = list(zip(eval_preds.preds, db_ids))
   references  = list(zip(gt_queries, db_ids))
   ```

   * `eval_preds.labels` ä¸å†å‚ä¸ reference æ„é€ ï¼›
   * reference æ°¸è¿œæ¥è‡ª canonical ground-truth SQLï¼ˆ`meta["query"]`ï¼‰ã€‚

---

## äº”ã€å‘åå…¼å®¹ä¸ç¼“å­˜ç­–ç•¥

* **æ—§ cache åœºæ™¯ï¼š**

  * æ—§ç‰ˆæœ¬æ„å»ºçš„ dataset cache ä¸åŒ…å« `meta["query"]`ï¼Œåœ¨ `compute_metrics` ä¸­ä¼šè¢«æ£€æµ‹åˆ°ã€‚
  * è¡Œä¸ºï¼šç«‹å³æŠ›å‡º `RuntimeError`ï¼Œæç¤ºæ¸…ç† `data/xlangai_spider_*/cache_*.pkl` æˆ– bump `DATA_CACHE_TAG`ã€‚
  * è®¾è®¡ç›®çš„ï¼šé¿å…â€œæ—§ cache + æ–°ä»£ç â€å¯¼è‡´æŒ‡æ ‡æ‚„æ‚„é”™ä½ã€‚

* **æ–° cache åœºæ™¯ï¼š**

  * åªè¦é‡æ–°æ„å»º SpiderDatasetï¼ˆæˆ–ä½¿ç”¨æ–°çš„ `DATA_CACHE_TAG`ï¼‰ï¼Œ`meta` ä¸­å°±ä¼šåŒ…æ‹¬ `query` å­—æ®µã€‚
  * æ­¤æ—¶ `compute_metrics` æ­£å¸¸è¿è¡Œï¼ŒSpider parser çœ‹åˆ°çš„æ˜¯å¹²å‡€ SQLï¼Œä¸å†è§¦å‘ `'select' not found'`ã€‚

---

## å…­ã€éªŒè¯æ­¥éª¤ï¼ˆç§’çº§ sanity checkï¼‰

åœ¨é…ç½®å¥½ Spider ç¯å¢ƒï¼ˆ`SPIDER_LOCAL_DIR`ã€`NLTK_DATA` ç­‰ï¼‰åï¼Œæ— éœ€å¯åŠ¨å®Œæ•´è®­ç»ƒï¼Œå³å¯å¿«é€ŸéªŒè¯æ•´æ¡ metric é“¾è·¯ï¼š

```bash
conda activate mzsz
cd /home/user/mzs_h/code/zh-LAT-peft/mamba-peft

python - <<'PY'
import os, sys, random
sys.path.insert(0, "/home/user/mzs_h/code/zh-LAT-peft/mamba-peft")

from transformers import AutoTokenizer
from dataset.spider_data import SpiderDataset
from metrics.spider.spider import SpiderMetric
from trainer.trainer_utils import MambaEvalPrediction

# 1) æ„é€  SpiderDatasetï¼ˆç”¨å’Œè®­ç»ƒä¸€è‡´çš„æœ¬åœ° JSONï¼‰
tok = AutoTokenizer.from_pretrained("facebook/opt-125m")
ds  = SpiderDataset(tok, split="train", has_test_split=True, use_cache=True)

print("len(ds) =", len(ds))
print("sample meta[0] =", ds.data[0][1])

# 2) æŠ½ N æ¡æ ·æœ¬ï¼Œæ„é€ â€œä¼ª eval_predsâ€
N = min(5, len(ds))
idxs = list(range(N))

# æ¨¡æ‹Ÿ evaluate_generation çš„è¾“å‡ºï¼špreds == labelsï¼ˆâ€œå®Œç¾é¢„æµ‹â€ï¼‰
input_ids = [ds.data[i][0][0] for i in idxs]
label_ids = [ds.data[i][0][1] for i in idxs]

eval_pred = MambaEvalPrediction(
    tokenizer=tok,
    input_ids=input_ids,
    pred_ids=label_ids,
    label_ids=label_ids,
    save_file=None,
    remove_eos=True,
)

metric = SpiderMetric()
db_ids = [ds.data[i][1]["db_id"] for i in idxs]
predictions = list(zip(eval_pred.preds, db_ids))
references  = list(zip([ds.data[i][1]["query"] for i in idxs], db_ids))

print("Sanity: first pred/ref pair:")
print("  pred =", predictions[0][0])
print("  ref  =", references[0][0])

out = metric.compute(predictions, references)
keys = [k for k in out.keys() if k.endswith("/exact")]
print("metric keys (sample):", keys[:5])
for k in sorted(keys)[:5]:
    print(k, "->", out[k])
PY
```

**é¢„æœŸï¼š**

* è‹¥ cache ä¸ºæ—§ç‰ˆæœ¬ï¼ˆæ—  `query`ï¼‰ï¼Œä¼šç›´æ¥æŠ›å‡º `RuntimeError`ï¼Œæç¤ºæ¸…ç†/é‡å»ºã€‚
* è‹¥ cache ä¸ºæ–°ç‰ˆæœ¬ï¼š

  * `len(ds)` ä¸ºæ­£ï¼›
  * `sample meta[0]` ä¸­åŒ…å« `{"db_id": ..., "query": "select ..."}`
  * `pred` ä¸ `ref` æ–‡æœ¬éå¸¸æ¥è¿‘ï¼ˆå› ä¸ºæˆ‘ä»¬æ¨¡æ‹Ÿäº†â€œå®Œç¾é¢„æµ‹â€ï¼‰ï¼›
  * `metric` è¾“å‡ºä¸­çš„ `*/exact` æŒ‡æ ‡æ¥è¿‘ 1ã€‚

ä¸€æ—¦è¿™ä¸ª sanity check é€šè¿‡ï¼Œå°±å¯ä»¥è¾ƒä¸ºæ”¾å¿ƒåœ°å¯åŠ¨å®Œæ•´ Spider è®­ç»ƒ/è¯„ä¼°æµç¨‹ï¼Œä¸å†æ‹…å¿ƒï¼š

* `'select' not found'` å´©æºƒï¼›
* æˆ– prediction/reference é”™ä½å¯¼è‡´çš„è¯¡å¼‚ metricã€‚

---

## ä¸ƒã€å½±å“é¢ä¸ç»“è®º

* **å½±å“ä»£ç èŒƒå›´ï¼š**

  * ä»…é™ `mamba-peft/dataset/spider_data.py` ä¸­ `SpiderDataset` çš„æ•°æ®æ„å»ºä¸ metrics è®¡ç®—é€»è¾‘ã€‚

* **æ¨¡å‹è¡Œä¸ºå½±å“ï¼š**

  * è®­ç»ƒé˜¶æ®µï¼ˆloss è®¡ç®—ï¼‰ä¸å˜ï¼›
  * ç”Ÿæˆé¢„æµ‹ï¼ˆpredictionsï¼‰ä¸å˜ï¼›
  * **ä»… evaluation é˜¶æ®µçš„ reference è·å–é€»è¾‘æ”¹å˜**ï¼š

    * ä» decoded labels â†’ æ”¹ä¸º HF Dataset ä¸­çš„ canonical ground-truth SQL (`query.lower().strip()`)

* **ä¿®å¤æ”¶ç›Šï¼š**

  1. å½»åº•æ¶ˆé™¤ Spider parser `'select' not found'` å´©æºƒã€‚
  2. é¿å…å› æ ·æœ¬è¿‡æ»¤é€ æˆçš„ç´¢å¼•é”™ä½ï¼Œä¿è¯ metrics å¯¹é½æ­£ç¡®ã€‚
  3. å¯¹æ—§ cache é‡‡ç”¨â€œæ˜¾å¼ fail fastâ€ï¼Œé˜²æ­¢é™é»˜é”™è¯¯ã€‚

æ•´ä½“æ¥çœ‹ï¼Œè¿™æ¬¡ä¿®å¤å°† Spider è¯„æµ‹é“¾è·¯ä»â€œè„†å¼± + æ˜“é”™ä½â€å‡çº§ä¸ºâ€œç´¢å¼•ä¸¥æ ¼å¯¹é½ + parser è¾“å…¥ç¨³å®šâ€ï¼Œå¯ä»¥å®‰å…¨æ”¯æ’‘åç»­å¤§è§„æ¨¡è®­ç»ƒä¸å¯¹æ¯”å®éªŒã€‚

  
### æœ€å°å¿…è¦æ¸…ç†

åœ¨ä½ å·²ç»ï¼š

```bash
cd /home/user/mzs_h/code/zh-LAT-peft/mamba-peft
rm -f data/xlangai_spider/cache_xlangai_spider-tvt_*_seqlen*.pkl
```

çš„åŸºç¡€ä¸Šï¼Œä¸ºäº† 100% ç¡®ä¿ç”¨åˆ°çš„æ˜¯**å¸¦ `query` meta çš„æ–° cache**ï¼Œå»ºè®®å†åšä¸¤ä»¶äº‹ï¼ˆäºŒé€‰ä¸€ï¼‰ï¼š

- **æ–¹æ¡ˆ Aï¼šä¸€æ¬¡æ€§å½»åº•åˆ  Spider çš„æ‰€æœ‰æ—§ cache**
  ```bash
  rm -f data/xlangai_spider/cache_*.pkl
  rm -f data/xlangai_spider/cache_*.pkl.tmp 2>/dev/null || true
  rm -f data/xlangai_spider/cache_*.pkl.lock 2>/dev/null || true
  ```
  è¿™æ ·æ— è®ºæ˜¯ `train` è¿˜æ˜¯ `val`ï¼Œéƒ½ä¼šåœ¨ä¸‹æ¬¡è¿è¡Œæ—¶ç”¨æ–°ä»£ç é‡å»ºã€‚
 