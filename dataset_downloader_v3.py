#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
dataset_downloader_v3.py
- æ‰«æå·²æœ‰ -> è·³è¿‡
- HuggingFace æ•°æ®é›†å¿«ç…§åˆ° <project>/data/<owner_repo>ï¼ˆæˆ– <project>/data/<repo>ï¼‰
- MNIST/CIFAR èµ° torchvisionï¼ˆå·²è£…æ‰ä¸‹ï¼‰
- Spider è‡ªåŠ¨è¡¥ tables.jsonï¼ˆä¼˜å…ˆ HFï¼Œå…¶æ¬¡ GitHubï¼Œæœ€å git clone å…œåº•ï¼‰
- 429/ç½‘ç»œé”™è¯¯è‡ªåŠ¨é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
- samsum è‡ªåŠ¨å›é€€åˆ° GEM/samsum
- å¤±è´¥ä¸ä¸­æ–­å¹¶è®°å½• log
"""

import argparse
import os
import sys
import time
import shutil
from pathlib import Path
from datetime import datetime
import traceback

# å¯é€‰ï¼šhf_transfer åŠ é€Ÿ
try:
    import hf_transfer  # noqa: F401
    os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "1")
except Exception:
    pass

from huggingface_hub import snapshot_download, hf_hub_download
from huggingface_hub.utils import HfHubHTTPError
import requests

_TV_OK = None  # torchvision lazy import flag


def log(msg, log_file):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    print(line, flush=True)
    if log_file:
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(line + "\n")


def dir_non_empty(p: Path) -> bool:
    return p.exists() and p.is_dir() and any(p.iterdir())


def ensure_parent(p: Path):
    p.mkdir(parents=True, exist_ok=True)


def _snapshot_with_retries(repo_id, repo_type, local_dir, max_retries=5, log_file=None):
    # æŒ‡æ•°é€€é¿ï¼š1s,2s,4s,8s,8s â€¦
    backoffs = [1, 2, 4, 8, 8]
    for i in range(max_retries):
        try:
            snapshot_download(
                repo_id=repo_id,
                repo_type=repo_type,
                local_dir=str(local_dir),
                local_dir_use_symlinks=False,  # æ–°ç‰ˆä¼šå¿½ç•¥ï¼Œä½†ä¿ç•™ä¸å½±å“
            )
            return True
        except Exception as e:
            msg = f"{type(e).__name__}: {e}"
            # 429/ç½‘ç»œå‹é”™è¯¯é‡è¯•
            if ("429" in msg or "Too Many Requests" in msg or
                "LocalEntryNotFoundError" in msg or
                isinstance(e, HfHubHTTPError)):
                if i < max_retries - 1:
                    t = backoffs[i]
                    log(f"âš ï¸  {repo_id} æ‹‰å–å¤±è´¥ï¼Œ{t}s åé‡è¯•ï¼ˆ{i+1}/{max_retries}ï¼‰ | {msg}", log_file)
                    time.sleep(t)
                    continue
            # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›
            raise
    return False


def download_hf_dataset(repo_ids, targets, log_file, resync=False):
    """
    repo_ids: [å€™é€‰repo id]ï¼ˆæ¯”å¦‚ ["samsum","GEM/samsum"]ï¼‰
    targets:  ä¸ repo_ids ä¸€ä¸€å¯¹åº”çš„ç›®æ ‡ç›®å½• Path
    è§„åˆ™ï¼š
      - åªè¦æœ‰ä¸€ä¸ª target ç›®å½•éç©ºï¼Œå°±è®¤ä¸ºâ€œå·²å­˜åœ¨â€ï¼Œç›´æ¥è·³è¿‡
      - å¦åˆ™æŒ‰é¡ºåºå°è¯•æ¯ä¸ª repo_idï¼ŒæˆåŠŸä¸€ä¸ªå³è¿”å› True
    """
    # å¦‚å·²æœ‰ä»»ä½•ä¸€ä¸ªå€™é€‰ç›®æ ‡ç›®å½•éç©º -> è·³è¿‡
    for t in targets:
        if dir_non_empty(t) and not resync:
            log(f"â­ï¸  skip (exists): {t}", log_file)
            return True

    # å¦åˆ™å°è¯•é€ä¸ª repo
    for rid, tgt in zip(repo_ids, targets):
        ensure_parent(tgt)
        try:
            log(f"â¬‡ï¸  snapshot {rid}  ->  {tgt}", log_file)
            _snapshot_with_retries(rid, "dataset", tgt, log_file=str(log_file))
            log(f"âœ… snapshot ok: {rid}", log_file)
            return True
        except Exception as e:
            log(f"âŒ snapshot failed: {rid} | {e}", log_file)
            log(traceback.format_exc(), log_file)
            # å°è¯•ä¸‹ä¸€ä¸ªå€™é€‰
            continue
    return False


def download_spider_tables(target_root: Path, log_file: str):
    """
    ç›®æ ‡ï¼š<data_root>/xlangai_spider/spider/tables.json
    å€™é€‰æ¥æºï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
      1) HF dataset taoyds/spider é‡Œçš„ spider/tables.json æˆ– tables.json / database/tables.json
      2) GitHub raw (main/master + 3ç§è·¯å¾„)
      3) git clone ä»“åº“åˆ°ä¸´æ—¶ç›®å½•åæŸ¥æ‰¾ tables.json
    """
    dst = target_root / "spider" / "tables.json"
    if dst.exists() and dst.stat().st_size > 100:  # ç²—ç•¥è§†ä¸ºæœ‰æ•ˆ
        log(f"â­ï¸  skip spider tables (exists): {dst}", log_file)
        return True

    ensure_parent(dst.parent)

    # 1) HF ä¼˜å…ˆ
    candidates = ["spider/tables.json", "tables.json", "database/tables.json"]
    for fname in candidates:
        try:
            log(f"â¬‡ï¸  spider tables via HF: taoyds/spider::{fname}", log_file)
            cached = hf_hub_download(
                repo_id="taoyds/spider",
                repo_type="dataset",
                filename=fname,
            )
            shutil.copyfile(cached, dst)
            log("âœ… spider tables.json ok (HF)", log_file)
            return True
        except Exception:
            pass

    # 2) GitHub raw
    for branch in ["main", "master"]:
        for path in candidates:
            url = f"https://raw.githubusercontent.com/taoyds/spider/{branch}/{path}"
            try:
                log(f"â¬‡ï¸  spider tables via GitHub: {url}", log_file)
                r = requests.get(url, timeout=60)
                r.raise_for_status()
                content = r.content
                # æçŸ­/HTML 404 æ–‡æœ¬è§†ä¸ºå¤±è´¥
                if len(content) < 100 or content.startswith(b"404"):
                    raise RuntimeError("content too small or 404 page")
                dst.write_bytes(content)
                log("âœ… spider tables.json ok (GitHub)", log_file)
                return True
            except Exception:
                continue

    # 3) git clone å…œåº•
    import tempfile, subprocess
    with tempfile.TemporaryDirectory() as tmp:
        try:
            log("â¬‡ï¸  spider tables via git clone ...", log_file)
            subprocess.run(
                ["git", "clone", "--depth", "1", "https://github.com/taoyds/spider", tmp],
                check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
            )
            found = None
            for rel in candidates:
                p = Path(tmp) / rel
                if p.exists():
                    found = p
                    break
            if found is None:
                # å†å…¨ç›˜æœ
                for p in Path(tmp).rglob("tables.json"):
                    found = p
                    break
            if found and found.stat().st_size > 100:
                shutil.copyfile(found, dst)
                log(f"âœ… spider tables.json ok (git clone at {found})", log_file)
                return True
            else:
                raise RuntimeError("tables.json not found after clone")
        except Exception as e:
            log(f"âŒ spider tables.json failed: {e}", log_file)
            log(traceback.format_exc(), log_file)
            return False


def _need_tv():
    global _TV_OK
    if _TV_OK is None:
        try:
            import torchvision  # noqa: F401
            _TV_OK = True
        except Exception:
            _TV_OK = False
    return _TV_OK


def download_mnist(target: Path, log_file: str):
    if not _need_tv():
        log("âŒ torchvision æœªå®‰è£…ï¼Œè·³è¿‡ MNISTï¼ˆpip install torchvisionï¼‰", log_file)
        return False
    try:
        from torchvision import datasets, transforms
        ensure_parent(target)
        log(f"â¬‡ï¸  MNIST -> {target}", log_file)
        datasets.MNIST(root=str(target), train=True, download=True, transform=transforms.ToTensor())
        datasets.MNIST(root=str(target), train=False, download=True, transform=transforms.ToTensor())
        log("âœ… MNIST ok", log_file)
        return True
    except Exception as e:
        log(f"âŒ MNIST failed: {e}", log_file)
        log(traceback.format_exc(), log_file)
        return False


def download_cifar10(target: Path, log_file: str):
    if not _need_tv():
        log("âŒ torchvision æœªå®‰è£…ï¼Œè·³è¿‡ CIFAR10ï¼ˆpip install torchvisionï¼‰", log_file)
        return False
    try:
        from torchvision import datasets, transforms
        ensure_parent(target)
        log(f"â¬‡ï¸  CIFAR10 -> {target}", log_file)
        datasets.CIFAR10(root=str(target), train=True, download=True, transform=transforms.ToTensor())
        datasets.CIFAR10(root=str(target), train=False, download=True, transform=transforms.ToTensor())
        log("âœ… CIFAR10 ok", log_file)
        return True
    except Exception as e:
        log(f"âŒ CIFAR10 failed: {e}", log_file)
        log(traceback.format_exc(), log_file)
        return False


def main():
    parser = argparse.ArgumentParser(description="Bulk dataset downloader (skip-if-exists, resilient).")
    parser.add_argument("--project", required=True, help="mamba-peft é¡¹ç›®æ ¹ç›®å½•ï¼ˆåŒ…å« data å­ç›®å½•ï¼‰")
    parser.add_argument("--only", nargs="*", default=None,
                        help="åªä¸‹è¿™äº›ï¼ˆç”¨ç©ºæ ¼æˆ–é€—å·åˆ†éš”ï¼‰ã€‚å¯ç”¨ job åï¼šalpaca/alpaca_eval/samsum/boolq/piqa/glue/arc/spider/mmlu/mnist/cifar10/dart æˆ– repo id å¦‚ GEM/dart")
    parser.add_argument("--resync", action="store_true", help="å³ä½¿ç›®å½•å­˜åœ¨ä¹Ÿåš snapshot æ ¡éªŒ/è¡¥é½")
    args = parser.parse_args()

    project = Path(args.project).expanduser().resolve()
    data_root = project / "data"
    ensure_parent(data_root)

    log_file = project / "download_datasets.log"
    log("========== start download ==========", str(log_file))

    jobs = []

    def add_hf(name, repo_id_or_list):
        if isinstance(repo_id_or_list, str):
            repo_ids = [repo_id_or_list]
        else:
            repo_ids = list(repo_id_or_list)
        targets = [data_root / rid.replace("/", "_") for rid in repo_ids]
        jobs.append(("hf", name, repo_ids, targets))

    def add_tv(name):
        jobs.append(("tv", name, None, None))

    # HF æ•°æ®é›†
    add_hf("alpaca",       "yahma/alpaca-cleaned")
    add_hf("alpaca_eval",  "tatsu-lab/alpaca_eval")
    add_hf("samsum",       ["samsum", "GEM/samsum"])   # è‡ªåŠ¨å›é€€
    add_hf("boolq",        "google/boolq")
    add_hf("piqa",         "piqa")
    add_hf("glue",         "nyu-mll/glue")
    add_hf("arc",          "allenai/ai2_arc")
    add_hf("spider",       "xlangai/spider")
    add_hf("mmlu",         "cais/mmlu")
    add_hf("dart",         "GEM/dart")                 # ä¿®æ­£ä¸º GEM/dart

    # torchvision
    add_tv("mnist")
    add_tv("cifar10")

    # å¤„ç† --onlyï¼ˆæ”¯æŒé€—å·/ç©ºæ ¼ & æ”¯æŒ job åæˆ– repo idï¼‰
    if args.only:
        raw = []
        for token in args.only:
            raw.extend([t for t in token.split(",") if t])
        only = set(raw)
        def keep(job):
            kind, name, repo_ids, targets = job
            if name in only:
                return True
            if kind == "hf" and any(r in only for r in repo_ids):
                return True
            return False
        jobs = [j for j in jobs if keep(j)]

    ok = 0
    fail = 0
    for kind, name, repo_ids, targets in jobs:
        try:
            if kind == "hf":
                if download_hf_dataset(repo_ids, targets, str(log_file), resync=args.resync):
                    # Spider è¿½åŠ  tables.json
                    if name == "spider":
                        if not download_spider_tables(targets[0], str(log_file)):
                            fail += 1
                            continue
                    ok += 1
                else:
                    fail += 1
            elif kind == "tv":
                if name == "mnist":
                    target = data_root / "mnist"
                    ok += 1 if download_mnist(target, str(log_file)) else 0
                    fail += 0 if target.exists() else 1
                elif name == "cifar10":
                    target = data_root / "cifar"
                    ok += 1 if download_cifar10(target, str(log_file)) else 0
                    fail += 0 if target.exists() else 1
                else:
                    log(f"âŒ unknown torchvision job: {name}", str(log_file))
                    fail += 1
            else:
                log(f"âŒ unknown job kind: {kind}", str(log_file))
                fail += 1
        except KeyboardInterrupt:
            log("ğŸ›‘ åœæ­¢ï¼ˆCtrl-Cï¼‰", str(log_file))
            break
        except Exception as e:
            log(f"âŒ job failed ({name}): {e}", str(log_file))
            log(traceback.format_exc(), str(log_file))
            fail += 1

    log(f"========== done ({ok} ok, {fail} fail) ==========", str(log_file))


if __name__ == "__main__":
    os.environ.setdefault("HF_HOME", str(Path.home() / ".cache" / "huggingface"))
    sys.exit(main())